{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb675d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcb5cd",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "same method as proposed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_from_dir(dir_path):\n",
    "  \"\"\"\n",
    "  Reads the texts from a given directory and saves them in the pd.DataFrame with columns ['id', 'file_1', 'file_2'].\n",
    "\n",
    "  Params:\n",
    "    dir_path (str): path to the directory with data\n",
    "  \"\"\"\n",
    "  # Count number of directories in the provided path\n",
    "  dir_count = sum(os.path.isdir(os.path.join(root, d)) for root, dirs, _ in os.walk(dir_path) for d in dirs)\n",
    "  data=[0 for _ in range(dir_count)]\n",
    "  print(f\"Number of directories: {dir_count}\")\n",
    "\n",
    "  # For each directory, read both file_1.txt and file_2.txt and save results to the list\n",
    "  i=0\n",
    "  for folder_name in sorted(os.listdir(dir_path)):\n",
    "    folder_path = os.path.join(dir_path, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "      try:\n",
    "        with open(os.path.join(folder_path, 'file_1.txt'), 'r', encoding='utf-8') as f1:\n",
    "          text1 = f1.read().strip()\n",
    "        with open(os.path.join(folder_path, 'file_2.txt'), 'r', encoding='utf-8') as f2:\n",
    "          text2 = f2.read().strip()\n",
    "        index = int(folder_name[-4:])\n",
    "        data[i]=(index, text1, text2)\n",
    "        i+=1\n",
    "      except Exception as e:\n",
    "        print(f\"Error reading directory {folder_name}: {e}\")\n",
    "\n",
    "  # Change list with results into pandas DataFrame\n",
    "  df = pd.DataFrame(data, columns=['id', 'file_1', 'file_2']).set_index('id')\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=\"/content/data/train\"\n",
    "df_train=read_texts_from_dir(train_path)\n",
    "test_path=\"/content/data/test\"\n",
    "df_test=read_texts_from_dir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc38f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_gt=pd.read_csv(\"/content/data/train.csv\")\n",
    "df_train_gt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa67e2",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ccfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e183e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTextManifoldDetector:\n",
    "    \"\"\"\n",
    "    Learn the manifold of real texts and detect fake texts as outliers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", device=None):\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Use a more reliable model for embeddings\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Add padding token if missing\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        print(f\"Model loaded successfully. Hidden size: {self.model.config.hidden_size}\")\n",
    "        print(f\"Number of layers: {self.model.config.num_hidden_layers}\")\n",
    "\n",
    "        # Initialize components\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "        self.outlier_detector = None\n",
    "        self.real_embeddings = None\n",
    "        self.real_centroid = None\n",
    "\n",
    "    def test_model(self, sample_text=\"This is a test sentence.\"):\n",
    "        \"\"\"\n",
    "        Test if the model works with a simple text\n",
    "        \"\"\"\n",
    "        print(\"Testing model with sample text...\")\n",
    "        try:\n",
    "            embeddings = self.get_all_layer_embeddings(sample_text)\n",
    "            print(f\"Model test successful!\")\n",
    "            print(f\"Number of layers: {len(embeddings)}\")\n",
    "            print(f\"Embedding shape for layer -1: {embeddings[-1].shape}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\" Model test failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_all_layer_embeddings(self, text, pooling='mean'):\n",
    "        \"\"\"\n",
    "        Extract embeddings from ALL layers\n",
    "\n",
    "        Returns:\n",
    "            dict: {layer_idx: embedding_tensor}\n",
    "        \"\"\"\n",
    "        # Ensure text is string and not empty\n",
    "        if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "            print(f\"Warning: Invalid text input: {text}\")\n",
    "            text = \"Empty text\"  # Fallback\n",
    "\n",
    "        model_inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Debug: Check token types\n",
    "        print(f\"Input IDs dtype: {model_inputs.input_ids.dtype}\")\n",
    "        print(f\"Input IDs shape: {model_inputs.input_ids.shape}\")\n",
    "\n",
    "        # Ensure input_ids are integers\n",
    "        if model_inputs.input_ids.dtype != torch.long:\n",
    "            model_inputs.input_ids = model_inputs.input_ids.long()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                outputs = self.model(\n",
    "                    input_ids=model_inputs.input_ids,\n",
    "                    attention_mask=model_inputs.attention_mask,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Model forward pass error: {e}\")\n",
    "                print(f\"Input shape: {model_inputs.input_ids.shape}\")\n",
    "                print(f\"Input sample: {model_inputs.input_ids[0][:10]}\")\n",
    "                raise e\n",
    "\n",
    "            layer_embeddings = {}\n",
    "            attention_mask = model_inputs.attention_mask.unsqueeze(-1)\n",
    "\n",
    "            for layer_idx, hidden_state in enumerate(outputs.hidden_states):\n",
    "                if pooling == 'mean':\n",
    "                    masked_embeddings = hidden_state * attention_mask\n",
    "                    embedding = masked_embeddings.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "                elif pooling == 'cls':\n",
    "                    embedding = hidden_state[:, 0, :]\n",
    "                elif pooling == 'max':\n",
    "                    embedding = torch.max(hidden_state, dim=1)[0]\n",
    "\n",
    "                layer_embeddings[layer_idx] = embedding.squeeze(0).cpu().numpy()\n",
    "\n",
    "        return layer_embeddings\n",
    "\n",
    "    def extract_real_texts(self, df, labels_df):\n",
    "        \"\"\"\n",
    "        Extract only the real texts from pairs using labels\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with text pairs (indexed by id)\n",
    "            labels_df: DataFrame with columns ['id', 'real_text_id'] where real_text_id is 1 or 2\n",
    "\n",
    "        Returns:\n",
    "            list: Real texts only\n",
    "        \"\"\"\n",
    "        real_texts = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            if idx in labels_df.index:\n",
    "                real_text_id = labels_df.loc[idx]['real_text_id']\n",
    "\n",
    "                if real_text_id == 1:\n",
    "                    real_texts.append(row['file_1'])\n",
    "                elif real_text_id == 2:\n",
    "                    real_texts.append(row['file_2'])\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid real_text_id {real_text_id} for index {idx}\")\n",
    "            else:\n",
    "                print(f\"Warning: No label found for index {idx}\")\n",
    "\n",
    "        print(f\"Extracted {len(real_texts)} real texts\")\n",
    "        return real_texts\n",
    "\n",
    "    def learn_real_manifold(self, real_texts, target_layer=-2):\n",
    "        \"\"\"\n",
    "        Learn the manifold of real texts\n",
    "\n",
    "        Args:\n",
    "            real_texts: List of real text strings\n",
    "            target_layer: Which layer to use for manifold learning\n",
    "        \"\"\"\n",
    "        print(f\"Learning manifold from {len(real_texts)} real texts...\")\n",
    "        print(f\"Using layer {target_layer}\")\n",
    "\n",
    "        # Extract embeddings from target layer\n",
    "        embeddings_list = []\n",
    "\n",
    "        for text in tqdm(real_texts, desc=\"Extracting embeddings\"):\n",
    "            layer_embeddings = self.get_all_layer_embeddings(text)\n",
    "            target_embedding = layer_embeddings[target_layer]\n",
    "            embeddings_list.append(target_embedding)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        self.real_embeddings = np.array(embeddings_list)\n",
    "        print(f\"Real embeddings shape: {self.real_embeddings.shape}\")\n",
    "\n",
    "        # Standardize embeddings\n",
    "        self.real_embeddings_scaled = self.scaler.fit_transform(self.real_embeddings)\n",
    "\n",
    "        # Apply PCA for dimensionality reduction\n",
    "        self.real_embeddings_pca = self.pca.fit_transform(self.real_embeddings_scaled)\n",
    "        print(f\"PCA reduced shape: {self.real_embeddings_pca.shape}\")\n",
    "        print(f\"Explained variance ratio: {self.pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "        # Compute centroid of real texts\n",
    "        self.real_centroid = np.mean(self.real_embeddings_pca, axis=0)\n",
    "\n",
    "        # Fit outlier detection model\n",
    "        print(\"Fitting outlier detection model...\")\n",
    "        self.outlier_detector = EllipticEnvelope(contamination=0.1, random_state=42)\n",
    "        self.outlier_detector.fit(self.real_embeddings_pca)\n",
    "\n",
    "        print(\" Real text manifold learned!\")\n",
    "\n",
    "    def predict_text(self, text, target_layer=-2):\n",
    "        \"\"\"\n",
    "        Predict if a text is real (1) or fake (0)\n",
    "\n",
    "        Args:\n",
    "            text: Input text string\n",
    "            target_layer: Same layer used for training\n",
    "\n",
    "        Returns:\n",
    "            int: 1 for real, 0 for fake\n",
    "            float: Distance from real centroid\n",
    "            float: Outlier score\n",
    "        \"\"\"\n",
    "        # Get embedding\n",
    "        layer_embeddings = self.get_all_layer_embeddings(text)\n",
    "        embedding = layer_embeddings[target_layer].reshape(1, -1)\n",
    "\n",
    "        # Transform using learned scaler and PCA\n",
    "        embedding_scaled = self.scaler.transform(embedding)\n",
    "        embedding_pca = self.pca.transform(embedding_scaled)\n",
    "\n",
    "        # Compute distance from centroid\n",
    "        distance = np.linalg.norm(embedding_pca - self.real_centroid)\n",
    "\n",
    "        # Predict using outlier detector\n",
    "        is_real = self.outlier_detector.predict(embedding_pca)[0]\n",
    "        outlier_score = self.outlier_detector.decision_function(embedding_pca)[0]\n",
    "\n",
    "        return int(is_real == 1), distance, outlier_score\n",
    "\n",
    "    def evaluate_pairs(self, df, labels_df, target_layer=-2):\n",
    "        \"\"\"\n",
    "        Evaluate on text pairs\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with text pairs\n",
    "            labels_df: DataFrame with columns ['id', 'real_text_id']\n",
    "\n",
    "        Returns:\n",
    "            np.array: Predictions (1 or 2 indicating which text is real)\n",
    "            dict: Detailed results\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        details = []\n",
    "\n",
    "        print(\"Evaluating text pairs...\")\n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            # Predict both texts\n",
    "            pred1, dist1, score1 = self.predict_text(row['file_1'], target_layer)\n",
    "            pred2, dist2, score2 = self.predict_text(row['file_2'], target_layer)\n",
    "\n",
    "            # Choose the one more likely to be real (higher score, lower distance)\n",
    "            if score1 > score2:  # Text 1 is more \"real-like\"\n",
    "                prediction = 1\n",
    "            else:  # Text 2 is more \"real-like\"\n",
    "                prediction = 2\n",
    "\n",
    "            predictions.append(prediction)\n",
    "\n",
    "            true_label = labels_df.loc[idx]['real_text_id'] if idx in labels_df.index else None\n",
    "            details.append({\n",
    "                'text1_score': score1,\n",
    "                'text2_score': score2,\n",
    "                'text1_distance': dist1,\n",
    "                'text2_distance': dist2,\n",
    "                'prediction': prediction,\n",
    "                'true_label': true_label\n",
    "            })\n",
    "\n",
    "        return np.array(predictions), details\n",
    "\n",
    "    def visualize_manifold(self, df, labels_df, target_layer=-2):\n",
    "        \"\"\"\n",
    "        Visualize the real text manifold and fake text positions\n",
    "        \"\"\"\n",
    "        if self.real_embeddings_pca.shape[1] < 2:\n",
    "            print(\"Need at least 2 PCA components for visualization\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot real text manifold\n",
    "        plt.scatter(self.real_embeddings_pca[:, 0], self.real_embeddings_pca[:, 1],\n",
    "                   alpha=0.6, c='green', label='Real Texts', s=50)\n",
    "\n",
    "        # Plot centroid\n",
    "        plt.scatter(self.real_centroid[0], self.real_centroid[1],\n",
    "                   c='red', s=200, marker='*', label='Real Centroid')\n",
    "\n",
    "        # Sample some fake texts and plot them\n",
    "        fake_embeddings = []\n",
    "        for idx, row in df.head(20).iterrows():  # Sample first 20\n",
    "            real_text_id = labels_df.loc[idx]['real_text_id']\n",
    "            fake_text = row['file_2'] if real_text_id == 1 else row['file_1']\n",
    "\n",
    "            layer_embeddings = self.get_all_layer_embeddings(fake_text)\n",
    "            embedding = layer_embeddings[target_layer].reshape(1, -1)\n",
    "            embedding_scaled = self.scaler.transform(embedding)\n",
    "            embedding_pca = self.pca.transform(embedding_scaled)\n",
    "            fake_embeddings.append(embedding_pca[0])\n",
    "\n",
    "        fake_embeddings = np.array(fake_embeddings)\n",
    "        plt.scatter(fake_embeddings[:, 0], fake_embeddings[:, 1],\n",
    "                   alpha=0.6, c='orange', label='Fake Texts', s=50)\n",
    "\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.title('Real vs Fake Text Manifold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "model_id = \"Qwen/Qwen2-1.5B\"\n",
    "model_id = \"Qwen/Qwen2-0.5B\"\n",
    "\n",
    "detector = RealTextManifoldDetector(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract real texts\n",
    "real_texts = detector.extract_real_texts(df_train, df_train_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.learn_real_manifold(real_texts, target_layer=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "predictions, details = detector.evaluate_pairs(df_train, df_train_gt,target_layer=15)\n",
    "true_labels = df_train_gt['real_text_id'].values\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"\\n🎯 Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_esa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
